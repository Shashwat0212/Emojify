{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xwTXmZ-cnnKUFxI0LBH74xDcnBQ3lZed",
      "authorship_tag": "ABX9TyPMs8134N8dRuPh6kTlH0ij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashwat0212/Emojify/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx32kQXzSxYv"
      },
      "source": [
        "!pip install face_recognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKYF4xv7dRQJ"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(r\"/content/drive/MyDrive/Colab Notebooks/Face-and-Emotion-Recognition/datasets.py\");\n",
        "sys.path.append(r\"/content/drive/MyDrive/Colab Notebooks/Face-and-Emotion-Recognition/inference.py\");\n",
        "sys.path.append(r\"/content/drive/MyDrive/Colab Notebooks/Face-and-Emotion-Recognition/preprocessor.py\");\n",
        "# sys.path.append(r\"D:\\Python35\\models\\slim\\datasets\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnQfKsp_hmRb"
      },
      "source": [
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvWR6pBmfzOq"
      },
      "source": [
        "import sys; sys.path.insert(0, '/content/drive/MyDrive/Colab Notebooks/Face-and-Emotion-Recognition/') # add parent folder path where lib folder is\n",
        "from datasets import get_labels\n",
        "from inference import detect_faces\n",
        "from inference import draw_text\n",
        "from inference import draw_bounding_box\n",
        "from inference import apply_offsets\n",
        "from inference import load_detection_model\n",
        "from preprocessor import preprocess_input # store_load is a file on my library folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxMDdHnLM_vD"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import dlib\n",
        "from imutils import face_utils\n",
        "import face_recognition\n",
        "from keras.models import load_model\n",
        "from statistics import mode\n",
        "\n",
        "USE_WEBCAM = False # If false, loads video file source\n",
        "\n",
        "# parameters for loading data and images\n",
        "emotion_model_path = '/content/drive/MyDrive/Colab Notebooks/Face-and-Emotion-Recognition/models/emotion_model.hdf5'\n",
        "emotion_labels = get_labels('fer2013')\n",
        "\n",
        "# hyper-parameters for bounding boxes shape\n",
        "frame_window = 10\n",
        "emotion_offsets = (20, 40)\n",
        "\n",
        "# loading models\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "emotion_classifier = load_model(emotion_model_path)\n",
        "\n",
        "# predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "\n",
        "# getting input model shapes for inference\n",
        "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
        "\n",
        "# starting lists for calculating modes\n",
        "emotion_window = []\n",
        "\n",
        "# starting video streaming\n",
        "\n",
        "cv2.namedWindow('window_frame')\n",
        "video_capture = cv2.VideoCapture(0)\n",
        "\n",
        "# Select video or webcam feed\n",
        "cap = None\n",
        "if (USE_WEBCAM == True):\n",
        "    cap = cv2.VideoCapture(0) # Webcam source\n",
        "else:\n",
        "    cap = cv2.VideoCapture('./test/testvdo.mp4') # Video file source\n",
        "\n",
        "while cap.isOpened(): # True:\n",
        "    ret, bgr_image = cap.read()\n",
        "\n",
        "    #bgr_image = video_capture.read()[1]\n",
        "\n",
        "    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
        "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    faces = detector(rgb_image)\n",
        "\n",
        "    for face_coordinates in faces:\n",
        "\n",
        "        x1, x2, y1, y2 = apply_offsets(face_utils.rect_to_bb(face_coordinates), emotion_offsets)\n",
        "        gray_face = gray_image[y1:y2, x1:x2]\n",
        "        try:\n",
        "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        gray_face = preprocess_input(gray_face, True)\n",
        "        gray_face = np.expand_dims(gray_face, 0)\n",
        "        gray_face = np.expand_dims(gray_face, -1)\n",
        "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
        "        emotion_probability = np.max(emotion_prediction)\n",
        "        emotion_label_arg = np.argmax(emotion_prediction)\n",
        "        emotion_text = emotion_labels[emotion_label_arg]\n",
        "        emotion_window.append(emotion_text)\n",
        "\n",
        "        if len(emotion_window) > frame_window:\n",
        "            emotion_window.pop(0)\n",
        "        try:\n",
        "            emotion_mode = mode(emotion_window)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        if emotion_text == 'angry':\n",
        "            color = emotion_probability * np.asarray((255, 0, 0))\n",
        "        elif emotion_text == 'sad':\n",
        "            color = emotion_probability * np.asarray((0, 0, 255))\n",
        "        elif emotion_text == 'happy':\n",
        "            color = emotion_probability * np.asarray((255, 255, 0))\n",
        "        elif emotion_text == 'surprise':\n",
        "            color = emotion_probability * np.asarray((0, 255, 255))\n",
        "        else:\n",
        "            color = emotion_probability * np.asarray((0, 255, 0))\n",
        "\n",
        "        color = color.astype(int)\n",
        "        color = color.tolist()\n",
        "\n",
        "        draw_bounding_box(face_utils.rect_to_bb(face_coordinates), rgb_image, color)\n",
        "        draw_text(face_utils.rect_to_bb(face_coordinates), rgb_image, emotion_mode,\n",
        "                  color, 0, -45, 1, 1)\n",
        "\n",
        "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imshow('window_frame', bgr_image)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows() \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}